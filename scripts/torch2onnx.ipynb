{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import torch, torchvision\n",
    "# import mxnet, mxnet.contrib.onnx\n",
    "# import mxnet.contrib.onnx.onnx2mx\n",
    "\n",
    "# model = torchvision.models.alexnet(pretrained=True)\n",
    "# model.train(False)\n",
    "\n",
    "# dummy_input = torch.autograd.Variable(torch.randn(1, 3, 224, 224))\n",
    "# torch.onnx.export(model, dummy_input, 'alexnet.onnx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%actual_input_1 : Float(1, 3, 224, 224)\n",
      "      %learned_0 : Float(64, 3, 11, 11)\n",
      "      %learned_1 : Float(64)\n",
      "      %learned_2 : Float(192, 64, 5, 5)\n",
      "      %learned_3 : Float(192)\n",
      "      %learned_4 : Float(384, 192, 3, 3)\n",
      "      %learned_5 : Float(384)\n",
      "      %learned_6 : Float(256, 384, 3, 3)\n",
      "      %learned_7 : Float(256)\n",
      "      %learned_8 : Float(256, 256, 3, 3)\n",
      "      %learned_9 : Float(256)\n",
      "      %learned_10 : Float(4096, 9216)\n",
      "      %learned_11 : Float(4096)\n",
      "      %learned_12 : Float(4096, 4096)\n",
      "      %learned_13 : Float(4096)\n",
      "      %learned_14 : Float(1000, 4096)\n",
      "      %learned_15 : Float(1000)) {\n",
      "  %17 : Float(1, 64, 55, 55) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0]\n",
      "  %18 : Float(1, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\n",
      "  %19 : Float(1, 64, 27, 27) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n",
      "  %20 : Float(1, 192, 27, 27) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%19, %learned_2, %learned_3), scope: AlexNet/Sequential[features]/Conv2d[3]\n",
      "  %21 : Float(1, 192, 27, 27) = onnx::Relu(%20), scope: AlexNet/Sequential[features]/ReLU[4]\n",
      "  %22 : Float(1, 192, 13, 13) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%21), scope: AlexNet/Sequential[features]/MaxPool2d[5]\n",
      "  %23 : Float(1, 384, 13, 13) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%22, %learned_4, %learned_5), scope: AlexNet/Sequential[features]/Conv2d[6]\n",
      "  %24 : Float(1, 384, 13, 13) = onnx::Relu(%23), scope: AlexNet/Sequential[features]/ReLU[7]\n",
      "  %25 : Float(1, 256, 13, 13) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%24, %learned_6, %learned_7), scope: AlexNet/Sequential[features]/Conv2d[8]\n",
      "  %26 : Float(1, 256, 13, 13) = onnx::Relu(%25), scope: AlexNet/Sequential[features]/ReLU[9]\n",
      "  %27 : Float(1, 256, 13, 13) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%26, %learned_8, %learned_9), scope: AlexNet/Sequential[features]/Conv2d[10]\n",
      "  %28 : Float(1, 256, 13, 13) = onnx::Relu(%27), scope: AlexNet/Sequential[features]/ReLU[11]\n",
      "  %29 : Float(1, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n",
      "  %30 : Dynamic = onnx::Shape(%29), scope: AlexNet\n",
      "  %31 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%30), scope: AlexNet\n",
      "  %32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet\n",
      "  %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet\n",
      "  %34 : Dynamic = onnx::Unsqueeze[axes=[0]](%32), scope: AlexNet\n",
      "  %35 : Dynamic = onnx::Unsqueeze[axes=[0]](%33), scope: AlexNet\n",
      "  %36 : Dynamic = onnx::Concat[axis=0](%34, %35), scope: AlexNet\n",
      "  %37 : Float(1, 9216) = onnx::Reshape(%29, %36), scope: AlexNet\n",
      "  %38 : Float(1, 9216), %39 : Dynamic = onnx::Dropout[is_test=1, ratio=0.5](%37), scope: AlexNet/Sequential[classifier]/Dropout[0]\n",
      "  %40 : Float(1, 4096) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%38, %learned_10, %learned_11), scope: AlexNet/Sequential[classifier]/Linear[1]\n",
      "  %41 : Float(1, 4096) = onnx::Relu(%40), scope: AlexNet/Sequential[classifier]/ReLU[2]\n",
      "  %42 : Float(1, 4096), %43 : Dynamic = onnx::Dropout[is_test=1, ratio=0.5](%41), scope: AlexNet/Sequential[classifier]/Dropout[3]\n",
      "  %44 : Float(1, 4096) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%42, %learned_12, %learned_13), scope: AlexNet/Sequential[classifier]/Linear[4]\n",
      "  %45 : Float(1, 4096) = onnx::Relu(%44), scope: AlexNet/Sequential[classifier]/ReLU[5]\n",
      "  %output1 : Float(1, 1000) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%45, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6]\n",
      "  return (%output1);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "model = torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "# Providing input and output names sets the display names for values\n",
    "# within the model's graph. Setting these does not change the semantics\n",
    "# of the graph; it is only for readability.\n",
    "#\n",
    "# The inputs to the network consist of the flat list of inputs (i.e.\n",
    "# the values you would pass to the forward() method) followed by the\n",
    "# flat list of parameters. You can partially specify names, i.e. provide\n",
    "# a list here shorter than the number of inputs to the model, and we will\n",
    "# only set that subset of names, starting from the beginning.\n",
    "input_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\n",
    "output_names = [ \"output1\" ]\n",
    "\n",
    "torch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.densenet121(pretrained=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
